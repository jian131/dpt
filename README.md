# CBIR Project - Team Guide ğŸ“š

**Äá» tÃ i:** Content-Based Image Retrieval (CBIR) + LSH Indexing
**NhÃ³m:** 4 thÃ nh viÃªn
**Dataset:** Fashion-MNIST (500 áº£nh, 10 classes)
**Repo:** https://github.com/jian131/dpt

---

## ğŸ“‹ Tá»•ng quan há»‡ thá»‘ng

### **Luá»“ng hoáº¡t Ä‘á»™ng chÃ­nh:**

```
1. BUILD PHASE (Offline - lÃ m 1 láº§n)
   Dataset (500 áº£nh)
      â†“
   Extract Features (HSV + LBP) â†’ features.npy (500 Ã— 2560)
      â†“
   Build LSH Index â†’ lsh_index.pkl (8 tables)
      â†“
   Save metadata â†’ meta.csv

2. SEARCH PHASE (Online - real-time)
   Query Image
      â†“
   Extract Features â†’ query vector (2560 dim)
      â†“
   LSH Query â†’ candidates (50-100 áº£nh)
      â†“
   Compute Distance (ChiÂ²) â†’ distances
      â†“
   Sort & Return Top-K â†’ results

3. EVALUATION PHASE
   Test vá»›i 50 queries
      â†“
   Compute Precision@K, Recall@K
      â†“
   Compare Linear vs LSH (speedup)
```

### **Cáº¥u trÃºc code:**

```
CBIR/
â”œâ”€â”€ config.py          (41 lines)  - Cáº¥u hÃ¬nh toÃ n bá»™ project
â”œâ”€â”€ features.py        (97 lines)  - Extract HSV + LBP features
â”œâ”€â”€ lsh.py             (83 lines)  - LSH indexing
â”œâ”€â”€ build.py           (93 lines)  - Build features + index
â”œâ”€â”€ search.py         (129 lines)  - Search vá»›i distance metrics
â”œâ”€â”€ eval.py           (110 lines)  - Evaluation
â”œâ”€â”€ gui.py            (228 lines)  - GUI demo (optional)
â””â”€â”€ requirements.txt               - Dependencies
```

**Tá»•ng code chÃ­nh: 553 dÃ²ng** (khÃ´ng ká»ƒ GUI)

---

## ğŸ‘¥ PHÃ‚N CÃ”NG 4 THÃ€NH VIÃŠN

### ğŸ‘¤ **THÃ€NH VIÃŠN 1: Feature Extraction (HSV Color)**

**File:** `features.py` - Part 1 (HSV histogram)
**DÃ²ng code:** ~60 lines

**Nhiá»‡m vá»¥:**

- Implement HSV Color Histogram vá»›i spatial grid 3Ã—3
- Quantization: H=16, S=4, V=4 bins
- Output: 2304-dim vector (9 cells Ã— 256 bins)

**BÃ¡o cÃ¡o:**

- Giáº£i thÃ­ch táº¡i sao dÃ¹ng HSV thay vÃ¬ RGB
- Demo histogram visualization
- So sÃ¡nh features giá»¯a 2 classes

---

### ğŸ‘¤ **THÃ€NH VIÃŠN 2: Feature Extraction (LBP Texture)**

**File:** `features.py` - Part 2 (LBP)
**DÃ²ng code:** ~40 lines

**Nhiá»‡m vá»¥:**

- Implement LBP (Local Binary Pattern) 3Ã—3 basic
- 8 neighbors encoding â†’ 256 patterns
- Output: 256-dim histogram

**BÃ¡o cÃ¡o:**

- Giáº£i thÃ­ch LBP encoding (binary pattern)
- Demo texture patterns khÃ¡c nhau
- So sÃ¡nh Ã¡o len vs Ã¡o lá»¥a

---

### ğŸ‘¤ **THÃ€NH VIÃŠN 3: LSH Indexing**

**File:** `lsh.py` (83 lines)

**Nhiá»‡m vá»¥:**

- Implement LSH vá»›i random hyperplanes
- Hash function: binary signature (12 bits)
- Multi-table (8 tables) Ä‘á»ƒ tÄƒng recall
- Build & query index

**BÃ¡o cÃ¡o:**

- Giáº£i thÃ­ch LSH theory (collision probability)
- Demo sá»‘ lÆ°á»£ng candidates: 80/500
- Complexity: O(k) vs O(n)

---

### ğŸ‘¤ **THÃ€NH VIÃŠN 4: Search, Distance Metrics & Evaluation**

**Files:** `search.py` (129 lines) + `eval.py` (110 lines) + `build.py` (93 lines)

**Nhiá»‡m vá»¥:**

- Implement 3 distance metrics: ChiÂ², L1, L2
- Linear search vs LSH search
- Build pipeline: dataset â†’ features â†’ index
- Evaluation: Precision@K, Recall@K, Speedup

**BÃ¡o cÃ¡o:**

- So sÃ¡nh 3 metrics (ChiÂ² tá»‘t nháº¥t)
- Speedup: Linear 28ms vs LSH 1.5ms â†’ 19x
- Precision/Recall curves

---

## ğŸ“Š PHÃ‚N Bá»” WORKLOAD

| ThÃ nh viÃªn      | Code (lines) | Äá»™ khÃ³   | Tasks                              |
| --------------- | ------------ | -------- | ---------------------------------- |
| 1 - HSV         | 60           | â­â­     | HSV quantization + Grid histogram  |
| 2 - LBP         | 40           | â­â­     | LBP encoding + Histogram           |
| 3 - LSH         | 83           | â­â­â­â­ | Random planes + Hash + Multi-table |
| 4 - Search/Eval | 332          | â­â­â­   | 3 Metrics + Search + Eval + Build  |

**Total: 515 lines thuáº­t toÃ¡n core**

---

## ğŸ‘¤ THÃ€NH VIÃŠN 1: HSV Color Histogram

### **Nhiá»‡m vá»¥ chi tiáº¿t:**

Implement thuáº­t toÃ¡n HSV Color Histogram vá»›i spatial grid

### **File phá»¥ trÃ¡ch:** `features.py` (lines 8-50)

---

#### **1.1. HSV Color Histogram**

**Táº¡i sao dÃ¹ng HSV thay vÃ¬ RGB?**

- **RGB:** Bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi Ã¡nh sÃ¡ng (sÃ¡ng/tá»‘i khÃ¡c nhau)
- **HSV:** TÃ¡ch mÃ u sáº¯c (H), Ä‘á»™ bÃ£o hÃ²a (S), Ä‘á»™ sÃ¡ng (V) â†’ á»•n Ä‘á»‹nh hÆ¡n

**Code chi tiáº¿t:**

```python
def hsv_quantize(hsv_img, bins_H, bins_S, bins_V):
    """
    Chuyá»ƒn áº£nh HSV thÃ nh bin indices

    Input:
        hsv_img: (H, W, 3) - áº£nh HSV
        bins_H=16, bins_S=4, bins_V=4

    Output:
        idx: (H, W) - má»—i pixel â†’ 1 sá»‘ (0-255)
    """
    H, W, _ = hsv_img.shape

    # TÃ¡ch 3 channels
    h = hsv_img[:, :, 0].astype(np.int32)  # [0,179]
    s = hsv_img[:, :, 1].astype(np.int32)  # [0,255]
    v = hsv_img[:, :, 2].astype(np.int32)  # [0,255]

    # Quantization: Chia khoáº£ng thÃ nh bins
    # VD: H=90 â†’ bin = 90*16/180 = 8
    bin_h = (h * bins_H // 180).clip(0, bins_H - 1)
    bin_s = (s * bins_S // 256).clip(0, bins_S - 1)
    bin_v = (v * bins_V // 256).clip(0, bins_V - 1)

    # Káº¿t há»£p 3 bins thÃ nh 1 index
    # Index = h*(S*V) + s*V + v
    # VD: (8,2,3) â†’ 8*(4*4) + 2*4 + 3 = 139
    return bin_h * (bins_S * bins_V) + bin_s * bins_V + bin_v
```

**Giáº£i thÃ­ch:**

- Chia má»—i channel thÃ nh bins: Hâ†’16, Sâ†’4, Vâ†’4
- Total bins = 16Ã—4Ã—4 = **256 bins**
- Má»—i pixel thuá»™c 1 bin â†’ táº¡o histogram

```python
def compute_grid_hsv_hist(img_bgr, grid, bins_H, bins_S, bins_V):
    """
    Compute spatial histogram (grid 3Ã—3)

    Táº¡i sao dÃ¹ng grid?
    - ToÃ n bá»™ áº£nh: Máº¥t thÃ´ng tin vá»‹ trÃ­
    - Grid 3Ã—3: Giá»¯ thÃ´ng tin "trÃªn/dÆ°á»›i/trÃ¡i/pháº£i"

    Output: 9 cells Ã— 256 bins = 2304 dim
    """
    gx, gy = grid  # (3, 3)
    H, W, _ = img_bgr.shape
    cell_h, cell_w = H // gy, W // gx
    K = bins_H * bins_S * bins_V  # 256

    hists = []
    for i in range(gy):  # 0,1,2
        for j in range(gx):  # 0,1,2
            # Cáº¯t áº£nh thÃ nh 9 cells
            y1, y2 = i * cell_h, (i + 1) * cell_h if i < gy - 1 else H
            x1, x2 = j * cell_w, (j + 1) * cell_w if j < gx - 1 else W
            cell = img_bgr[y1:y2, x1:x2]

            # Compute histogram cho cell nÃ y
            hists.append(compute_hsv_hist(cell, bins_H, bins_S, bins_V))

    return np.concatenate(hists)  # [2304]
```

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

```
áº¢nh Ã¡o Ä‘á» (256Ã—256):
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ Äá»  â”‚ Äá»  â”‚ Äá»  â”‚  â† Top row: mÃ u Ä‘á» dominant
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚Tráº¯ngâ”‚Tráº¯ngâ”‚Tráº¯ngâ”‚  â† Middle: mÃ u tráº¯ng
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ Äá»  â”‚ Äá»  â”‚ Äá»  â”‚  â† Bottom: mÃ u Ä‘á»
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

â†’ 9 histograms riÃªng biá»‡t
â†’ PhÃ¢n biá»‡t Ä‘Æ°á»£c "Ã¡o Ä‘á» viá»n tráº¯ng" vs "Ã¡o tráº¯ng viá»n Ä‘á»"
```

**Demo cho ThÃ nh viÃªn 1:**

**1. Visualize HSV histogram:**

```python
import matplotlib.pyplot as plt

img_path = "dataset/T-shirt/0.jpg"
img = cv2.imread(img_path)
img = cv2.resize(img, (256, 256))

# Compute histogram cho 1 cell
hsv = cv2.cvtColor(img[:85, :85], cv2.COLOR_BGR2HSV)
idx = hsv_quantize(hsv, 16, 4, 4)
hist = np.bincount(idx.ravel(), minlength=256)

# Plot
plt.bar(range(256), hist)
plt.title("HSV Histogram - Top-left cell")
plt.xlabel("Bin")
plt.ylabel("Frequency")
plt.show()
```

**2. So sÃ¡nh 2 classes:**

```python
# T-shirt (xÃ¡m) vs Dress (tráº¯ng)
t_shirt = compute_grid_hsv_hist(cv2.imread("dataset/T-shirt/0.jpg"), (3,3), 16,4,4)
dress = compute_grid_hsv_hist(cv2.imread("dataset/Dress/0.jpg"), (3,3), 16,4,4)

# Cosine similarity
sim = np.dot(t_shirt, dress) / (np.linalg.norm(t_shirt) * np.linalg.norm(dress))
print(f"Similarity: {sim:.3f}")  # Low (~0.3-0.4)
```

**3. Spatial information:**

```python
# So sÃ¡nh global vs spatial
global_hist = compute_hsv_hist(img, 16, 4, 4)  # 256 dim
spatial_hist = compute_grid_hsv_hist(img, (3,3), 16,4,4)  # 2304 dim

print(f"Global: {global_hist.shape}")    # (256,)
print(f"Spatial: {spatial_hist.shape}")  # (2304,)
# Spatial giá»¯ Ä‘Æ°á»£c thÃ´ng tin vá»‹ trÃ­ mÃ u sáº¯c!
```

---

## ğŸ‘¤ THÃ€NH VIÃŠN 2: LBP Texture

### **Nhiá»‡m vá»¥ chi tiáº¿t:**

Implement Local Binary Pattern Ä‘á»ƒ capture texture

### **File phá»¥ trÃ¡ch:** `features.py` (lines 51-76)

---

#### **2.1. LBP Theory**

**Code chi tiáº¿t:**

```python
def compute_lbp_hist(gray_img):
    """
    Local Binary Pattern - MÃ£ hÃ³a texture

    CÃ¡ch hoáº¡t Ä‘á»™ng:
    1. Láº¥y 1 pixel lÃ m center
    2. So sÃ¡nh vá»›i 8 neighbors xung quanh
    3. Táº¡o binary code (8 bits) â†’ 1 sá»‘ (0-255)

    Input: gray_img (H, W) - áº£nh grayscale
    Output: histogram (256,) - phÃ¢n bá»‘ LBP codes
    """
    H, W = gray_img.shape
    lbp = np.zeros((H - 2, W - 2), dtype=np.uint8)

    # 8 hÆ°á»›ng: â†– â†‘ â†— â†’ â†˜ â†“ â†™ â†
    neighbors = [(-1,-1), (-1,0), (-1,1), (0,1),
                 (1,1), (1,0), (1,-1), (0,-1)]

    for y in range(1, H - 1):
        for x in range(1, W - 1):
            center = gray_img[y, x]  # Pixel trung tÃ¢m
            code = 0

            # So sÃ¡nh vá»›i 8 neighbors
            for k, (dy, dx) in enumerate(neighbors):
                neighbor = gray_img[y + dy, x + dx]

                # Náº¿u neighbor >= center â†’ bit = 1
                if neighbor >= center:
                    code |= (1 << k)  # Set bit thá»© k

            lbp[y - 1, x - 1] = code  # Code tá»« 0-255

    # Táº¡o histogram
    hist = np.bincount(lbp.ravel(), minlength=256).astype(np.float32)
    return hist / (hist.sum() + 1e-12)  # Normalize
```

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

```
       50  60  70
       40 [55] 65   â† Center pixel = 55
       30  45  75

So sÃ¡nh:
â†– 50 < 55 â†’ 0
â†‘ 60 > 55 â†’ 1
â†— 70 > 55 â†’ 1
â†’ 65 > 55 â†’ 1
â†˜ 75 > 55 â†’ 1
â†“ 45 < 55 â†’ 0
â†™ 30 < 55 â†’ 0
â† 40 < 55 â†’ 0

Binary: 01111000 â†’ Decimal: 120
â†’ LBP code = 120
```

**Ã nghÄ©a:**

- Code 120 xuáº¥t hiá»‡n nhiá»u â†’ Texture cÃ³ pattern cá»¥ thá»ƒ
- Má»—i texture khÃ¡c nhau â†’ Histogram khÃ¡c nhau
- VÃ­ dá»¥:
  - Ão len: Nhiá»u codes 11111111, 00000000 (thÃ´ rÃ¡p)
  - Ão lá»¥a: Nhiá»u codes 01010101 (má»‹n mÃ ng)

**Demo cho ThÃ nh viÃªn 2:**

**1. Visualize LBP codes:**

```python
img = cv2.imread("dataset/Coat/0.jpg", cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, (256, 256))

# Compute LBP
lbp_hist = compute_lbp_hist(img)

# Plot histogram
plt.bar(range(256), lbp_hist)
plt.title("LBP Histogram - Coat texture")
plt.xlabel("LBP Code (0-255)")
plt.ylabel("Frequency")
plt.show()
```

**2. So sÃ¡nh textures:**

```python
# Ão len (thÃ´) vs Ão lá»¥a (má»‹n)
coat_gray = cv2.imread("dataset/Coat/0.jpg", cv2.IMREAD_GRAYSCALE)
dress_gray = cv2.imread("dataset/Dress/0.jpg", cv2.IMREAD_GRAYSCALE)

coat_lbp = compute_lbp_hist(coat_gray)
dress_lbp = compute_lbp_hist(dress_gray)

# Compare
plt.subplot(1,2,1)
plt.bar(range(256), coat_lbp)
plt.title("Coat (rough)")

plt.subplot(1,2,2)
plt.bar(range(256), dress_lbp)
plt.title("Dress (smooth)")
plt.show()
```

**3. LBP image visualization:**

```python
# Visualize LBP values
H, W = img.shape
lbp_img = np.zeros((H-2, W-2), dtype=np.uint8)
# ... (compute LBP for each pixel)

plt.subplot(1,2,1)
plt.imshow(img, cmap='gray')
plt.title("Original")

plt.subplot(1,2,2)
plt.imshow(lbp_img, cmap='gray')
plt.title("LBP codes")
plt.show()
```

---

## ğŸ‘¤ THÃ€NH VIÃŠN 3: LSH Indexing

### **Nhiá»‡m vá»¥ chi tiáº¿t:**

Implement Locality-Sensitive Hashing vá»›i random hyperplanes

### **File phá»¥ trÃ¡ch:** `lsh.py` (83 lines)

    Output: (2560,) = 2304 (HSV) + 256 (LBP)
    """
    img = cv2.imread(img_path)
    if img is None:
        raise ValueError(f"Cannot read: {img_path}")

    img = cv2.resize(img, config.image_size)  # 256Ã—256
    features = []

    if use_color:
        # HSV histogram: 2304 dim
        color_vec = compute_grid_hsv_hist(
            img, config.grid, config.bins_H, config.bins_S, config.bins_V
        )
        features.append(color_vec * config.w_color)  # Weight: 0.6

    if use_lbp:
        # LBP texture: 256 dim
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        lbp_vec = compute_lbp_hist(gray)
        features.append(lbp_vec * config.w_lbp)  # Weight: 0.4

    if not features:
        raise ValueError("Must enable at least one feature")

    # Káº¿t há»£p
    vec = np.concatenate(features)  # [2560]

    # L2 normalization: ÄÆ°a vá» unit vector
    vec = vec / (np.linalg.norm(vec) + 1e-12)

    return vec.astype(np.float32)

````

**Táº¡i sao normalize?**

- Äáº£m báº£o táº¥t cáº£ vectors cÃ³ Ä‘á»™ dÃ i = 1
- Distance metrics fair hÆ¡n (khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi magnitude)

---

### **Demo cho ThÃ nh viÃªn 1:**

**1. Visualize histogram:**

```python
import matplotlib.pyplot as plt

img_path = "dataset/T-shirt/0.jpg"
feat = extract_feature(img_path, config)

# Plot HSV histogram
plt.subplot(1,2,1)
plt.bar(range(256), feat[:256])
plt.title("HSV Histogram (Cell 1)")

# Plot LBP histogram
plt.subplot(1,2,2)
plt.bar(range(256), feat[2304:])
plt.title("LBP Histogram")
plt.show()
````

**2. So sÃ¡nh features:**

```python
# T-shirt vs Trouser
t_shirt = extract_feature("dataset/T-shirt/0.jpg", config)
trouser = extract_feature("dataset/Trouser/0.jpg", config)

# Cosine similarity
sim = np.dot(t_shirt, trouser)
print(f"Similarity: {sim:.3f}")  # ~0.3-0.4 (khÃ¡c nhau)

# T-shirt vs T-shirt khÃ¡c
t_shirt2 = extract_feature("dataset/T-shirt/1.jpg", config)
sim2 = np.dot(t_shirt, t_shirt2)
print(f"Similarity: {sim2:.3f}")  # ~0.7-0.9 (giá»‘ng nhau)
```

---

## ğŸ‘¤ THÃ€NH VIÃŠN 2: LSH Indexing

### **Nhiá»‡m vá»¥:**

Implement Locality-Sensitive Hashing Ä‘á»ƒ tÄƒng tá»‘c search:

- Random hyperplanes projection
- Multi-table hashing
- Query candidates retrieval

### **File phá»¥ trÃ¡ch:** `lsh.py`

---

#### **2.1. LÃ½ thuyáº¿t LSH**

**Váº¥n Ä‘á»:**

- Database cÃ³ 500 áº£nh
- Linear search: So sÃ¡nh query vá»›i **500 áº£nh** â†’ O(n)
- Náº¿u n lá»›n (1M áº£nh) â†’ quÃ¡ cháº­m!

**Giáº£i phÃ¡p: LSH**

- Chia khÃ´ng gian thÃ nh "buckets" (nhÃ³m)
- áº¢nh giá»‘ng nhau â†’ cÃ¹ng bucket (high probability)
- Search chá»‰ trong bucket â†’ O(k) vá»›i k << n

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```
KhÃ´ng gian 2560 chiá»u
        â†“
Random Hyperplanes (12 planes)
        â†“
Binary Hash Code (12 bits)
        â†“
Bucket ID (0-4095)
        â†“
Hash Table (dict)
```

**VÃ­ dá»¥ 2D:**

```
       |
   â—   |   â—‹
   â— â— | â—‹ â—‹
â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€â”€â”€â”€â”€
   â—   |   â—‹
       |

Plane chia khÃ´ng gian thÃ nh 2 pháº§n:
- TrÃ¡i: â— (similar items)
- Pháº£i: â—‹ (similar items)

Vá»›i 2 planes â†’ 2Â² = 4 buckets
Vá»›i 12 planes â†’ 2Â¹Â² = 4096 buckets
```

---

#### **2.2. Code chi tiáº¿t**

```python
class LSHIndex:
    def __init__(self, num_tables, num_planes, dim, seed=42):
        """
        LSH Index vá»›i random hyperplanes

        Args:
            num_tables: Sá»‘ lÆ°á»£ng hash tables (8)
            num_planes: Sá»‘ planes má»—i table (12)
            dim: Feature dimension (2560)
            seed: Random seed
        """
        self.num_tables = num_tables
        self.num_planes = num_planes
        self.dim = dim
        self.seed = seed

        # Generate random planes
        self.planes = self._make_planes()

        # Hash tables (8 tables)
        self.tables = [dict() for _ in range(num_tables)]

        self.num_vectors = 0
```

**Táº¡i sao 8 tables?**

- 1 table cÃ³ thá»ƒ miss má»™t sá»‘ áº£nh tÆ°Æ¡ng tá»±
- 8 tables â†’ 8 láº§n random â†’ tÄƒng recall
- Trade-off: Nhiá»u tables â†’ cháº­m hÆ¡n

---

#### **2.3. Random Hyperplanes**

```python
def _make_planes(self):
    """
    Táº¡o random hyperplanes

    Hyperplane: axâ‚ + bxâ‚‚ + ... + cÂ·xâ‚‚â‚…â‚†â‚€ = 0
    Represented by vector (a, b, ..., c)

    Output: 8 tables Ã— 12 planes = 96 vectors (12, 2560)
    """
    np.random.seed(self.seed)
    planes = []

    for _ in range(self.num_tables):
        # Random normal distribution
        p = np.random.randn(self.num_planes, self.dim).astype(np.float32)

        # Normalize to unit vectors
        p = p / (np.linalg.norm(p, axis=1, keepdims=True) + 1e-12)

        planes.append(p)

    return planes
```

**Táº¡i sao normalize?**

- Äáº£m báº£o chá»‰ quan tÃ¢m Ä‘áº¿n **direction**, khÃ´ng pháº£i magnitude
- Dot product = cosine similarity

---

#### **2.4. Hash Function**

```python
def _hash(self, vec, planes):
    """
    Hash vector thÃ nh binary code

    Steps:
    1. Dot product vá»›i 12 planes
    2. Náº¿u > 0 â†’ bit = 1, else â†’ bit = 0
    3. Káº¿t há»£p 12 bits thÃ nh 1 sá»‘ (0-4095)

    Args:
        vec: (2560,) - feature vector
        planes: (12, 2560) - random hyperplanes

    Returns:
        hash_val: 0-4095 (12-bit number)
    """
    # Dot product vá»›i táº¥t cáº£ planes
    dots = np.dot(planes, vec)  # (12,)

    # Threshold táº¡i 0
    bits = (dots >= 0).astype(np.uint8)  # [1,0,1,1,0,...]

    # Convert binary to decimal
    hash_val = 0
    for i, bit in enumerate(bits):
        if bit:
            hash_val |= (1 << i)  # Set bit thá»© i

    return hash_val
```

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

```python
vec = [0.1, 0.5, -0.3, ..., 0.2]  # 2560 dim

planes = [
    [0.2, 0.1, ...],  # plane 1
    [-0.1, 0.3, ...], # plane 2
    ...
]

dots = [0.15, -0.05, 0.23, ...]  # 12 values

bits:
plane 1:  0.15 > 0 â†’ 1
plane 2: -0.05 < 0 â†’ 0
plane 3:  0.23 > 0 â†’ 1
...
â†’ [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1]

Binary: 101101001101
Decimal: 2893

â†’ hash_val = 2893
```

---

#### **2.5. Build Index**

```python
def fit(self, vectors):
    """
    Build hash tables tá»« database

    Args:
        vectors: (500, 2560) - táº¥t cáº£ features
    """
    self.num_vectors = len(vectors)
    self.tables = [dict() for _ in range(self.num_tables)]

    # Hash tá»«ng vector vÃ o 8 tables
    for vid in range(len(vectors)):
        for tid in range(self.num_tables):
            # Hash vector nÃ y
            h = self._hash(vectors[vid], self.planes[tid])

            # Add vÃ o bucket
            if h not in self.tables[tid]:
                self.tables[tid][h] = []
            self.tables[tid][h].append(vid)
```

**VÃ­ dá»¥ sau khi build:**

```python
tables[0] = {
    2893: [0, 15, 234],     # Bucket 2893 cÃ³ 3 áº£nh
    1024: [1, 2, 88, 99],   # Bucket 1024 cÃ³ 4 áº£nh
    ...
}

tables[1] = {
    567: [0, 10, 20],
    ...
}
```

**Observation:**

- áº¢nh giá»‘ng nhau (similar features) â†’ same hash code
- RÆ¡i vÃ o cÃ¹ng bucket!

---

#### **2.6. Query**

```python
def query(self, vec):
    """
    TÃ¬m candidates cho query vector

    Steps:
    1. Hash query vÃ o 8 tables
    2. Láº¥y union táº¥t cáº£ buckets
    3. Return candidates

    Returns:
        set of image IDs (50-100 candidates)
    """
    candidates = set()

    # Query tá»«ng table
    for tid in range(self.num_tables):
        # Hash query
        h = self._hash(vec, self.planes[tid])

        # Láº¥y bucket nÃ y
        if h in self.tables[tid]:
            candidates.update(self.tables[tid][h])

    return candidates
```

**VÃ­ dá»¥:**

```python
query_vec = [...]  # T-shirt features

# Hash vÃ o 8 tables
table 0: hash = 2893 â†’ bucket cÃ³ [0, 15, 234]
table 1: hash = 567  â†’ bucket cÃ³ [0, 10, 20]
table 2: hash = 3012 â†’ bucket cÃ³ [15, 99]
...

# Union táº¥t cáº£
candidates = {0, 10, 15, 20, 99, 234, ...}
â†’ ~80 candidates (thay vÃ¬ 500!)
```

**Speedup:**

- Linear: So sÃ¡nh vá»›i 500 áº£nh
- LSH: So sÃ¡nh vá»›i 80 áº£nh â†’ **6x nhanh hÆ¡n**
- Thá»±c táº¿: 19-22x (vÃ¬ LSH query cÅ©ng nhanh)

---

### **Demo cho ThÃ nh viÃªn 3:**

**1. Collision probability test:**

```python
# Táº¡o 2 vectors tÆ°Æ¡ng tá»± 90%
v1 = np.random.randn(2560)
v2 = 0.9 * v1 + 0.1 * np.random.randn(2560)
v1 = v1 / np.linalg.norm(v1)
v2 = v2 / np.linalg.norm(v2)

# Build index vá»›i v1
index = LSHIndex(8, 12, 2560, seed=42)
index.fit(np.array([v1]))

# Query vá»›i v2
candidates = index.query(v2)
print(f"v2 â†’ v1 collision: {0 in candidates}")  # True vá»›i high probability

# Test vá»›i vector random (khÃ´ng giá»‘ng)
v3 = np.random.randn(2560)
v3 = v3 / np.linalg.norm(v3)
candidates3 = index.query(v3)
print(f"v3 â†’ v1 collision: {0 in candidates3}")  # False
```

**2. Candidates reduction:**

```python
# Load features
features = np.load("artifacts/features.npy")  # (500, 2560)

# Build index
index = LSHIndex(8, 12, 2560)
index.fit(features)

# Query nhiá»u áº£nh
num_candidates = []
for i in range(50):
    cands = index.query(features[i])
    num_candidates.append(len(cands))

print(f"Avg candidates: {np.mean(num_candidates):.0f}/{len(features)}")  # ~80/500
print(f"Reduction: {len(features) / np.mean(num_candidates):.1f}x")      # ~6.3x
```

**3. Hash distribution:**

```python
# PhÃ¢n bá»‘ hash codes trong 1 table
hash_counts = {}
for i in range(len(features)):
    h = index._hash(features[i], index.planes[0])
    hash_counts[h] = hash_counts.get(h, 0) + 1

# Plot distribution
import matplotlib.pyplot as plt
plt.hist(hash_counts.values(), bins=20)
plt.xlabel("Bucket size")
plt.ylabel("Frequency")
plt.title("Hash distribution (Table 0)")
plt.show()
```

---

## ğŸ‘¤ THÃ€NH VIÃŠN 4: Search, Distance Metrics & Evaluation

### **Nhiá»‡m vá»¥ chi tiáº¿t:**

Implement search pipeline, distance metrics, vÃ  evaluation metrics

### **Files phá»¥ trÃ¡ch:**

- `search.py` (129 lines) - Distance metrics + Search algorithms
- `eval.py` (110 lines) - Evaluation metrics
- `build.py` (93 lines) - Build pipeline

**2. Sá»‘ lÆ°á»£ng candidates:**

```python
# Build index vá»›i 500 áº£nh
features = np.load("artifacts/features.npy")
index = LSHIndex(8, 12, 2560)
index.fit(features)

# Query 50 áº£nh
num_candidates = []
for i in range(50):
    cands = index.query(features[i])
    num_candidates.append(len(cands))

print(f"Avg candidates: {np.mean(num_candidates):.0f}")  # ~80
print(f"Reduction: {500 / np.mean(num_candidates):.1f}x")  # ~6x
```

---

## ğŸ‘¤ THÃ€NH VIÃŠN 3: Search & Distance Metrics

### **Nhiá»‡m vá»¥:**

- Implement 3 distance metrics: ChiÂ², L1, L2
- Linear search vs LSH search
- Build features tá»« dataset

### **File phá»¥ trÃ¡ch:** `search.py`, `build.py`

---

#### **3.1. Distance Metrics**

**Táº¡i sao cáº§n distance?**

- Features lÃ  vectors (2560 dim)
- Cáº§n Ä‘o "Ä‘á»™ khÃ¡c biá»‡t" giá»¯a 2 vectors
- Distance nhá» â†’ similar, distance lá»›n â†’ different

---

##### **Chi-Square Distance**

```python
def chi2_distance(a, b, eps=1e-10):
    """
    Chi-square distance cho histograms

    Formula: Ï‡Â² = 0.5 Ã— Î£ [(aáµ¢ - báµ¢)Â² / (aáµ¢ + báµ¢)]

    Táº¡i sao dÃ¹ng cho histogram?
    - Normalize by sum (aáµ¢ + báµ¢) â†’ robust
    - KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi magnitude

    Args:
        a, b: (2560,) hoáº·c (N, 2560)

    Returns:
        distance: scalar hoáº·c (N,)
    """
    diff = a - b  # Element-wise difference
    sum_ab = a + b + eps  # TrÃ¡nh chia 0

    # ChiÂ² formula
    chi2 = 0.5 * np.sum((diff * diff) / sum_ab, axis=-1)

    return chi2
```

**VÃ­ dá»¥:**

```python
# Histogram A: [0.5, 0.3, 0.2]
# Histogram B: [0.4, 0.4, 0.2]

diff = [0.1, -0.1, 0.0]
sum = [0.9, 0.7, 0.4]

chi2 = 0.5 * (0.1Â²/0.9 + 0.1Â²/0.7 + 0Â²/0.4)
     = 0.5 * (0.011 + 0.014 + 0)
     = 0.0125
```

**Æ¯u Ä‘iá»ƒm:**

- Tá»‘t cho histogram comparison
- Robust vá»›i outliers

---

##### **L1 Distance (Manhattan)**

```python
def l1_distance(a, b):
    """
    L1 (Manhattan) distance

    Formula: L1 = Î£ |aáµ¢ - báµ¢|

    Ã nghÄ©a:
    - Tá»•ng absolute differences
    - "Khoáº£ng cÃ¡ch Ä‘i trÃªn lÆ°á»›i Ã´"
    """
    return np.sum(np.abs(a - b), axis=-1)
```

**VÃ­ dá»¥:**

```python
a = [1, 2, 3]
b = [2, 1, 4]

L1 = |1-2| + |2-1| + |3-4|
   = 1 + 1 + 1
   = 3
```

---

##### **L2 Distance (Euclidean)**

```python
def l2_distance(a, b):
    """
    L2 (Euclidean) distance

    Formula: L2 = âˆš(Î£ (aáµ¢ - báµ¢)Â²)

    Ã nghÄ©a:
    - "Khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng"
    - Phá»• biáº¿n nháº¥t
    """
    return np.sqrt(np.sum((a - b) ** 2, axis=-1))
```

**VÃ­ dá»¥:**

```python
a = [1, 2]
b = [4, 6]

L2 = âˆš((1-4)Â² + (2-6)Â²)
   = âˆš(9 + 16)
   = âˆš25
   = 5
```

---

##### **So sÃ¡nh 3 metrics:**

```python
a = np.array([0.5, 0.3, 0.2])
b = np.array([0.4, 0.4, 0.2])

print(f"ChiÂ²: {chi2_distance(a, b):.4f}")  # 0.0125
print(f"L1:   {l1_distance(a, b):.4f}")    # 0.2
print(f"L2:   {l2_distance(a, b):.4f}")    # 0.1414
```

**Khi nÃ o dÃ¹ng metric nÃ o?**

- **ChiÂ²:** Histogram features (HSV, LBP) âœ… Tá»‘t nháº¥t cho CBIR
- **L1:** Simple, fast
- **L2:** General purpose

---

#### **3.2. Linear Search**

```python
def search_linear(query_vec, features, metric, k):
    """
    Linear search - brute force

    Steps:
    1. Compute distance vá»›i Táº¤T Cáº¢ 500 áº£nh
    2. Sort distances
    3. Return top-K smallest

    Complexity: O(n) vá»›i n=500
    """
    start = time.time()

    # Compute distances (vectorized)
    distances = pairwise_distance(query_vec, features, metric)
    # distances: (500,)

    # Get top-K indices
    topk_ids = topk_indices(distances, k)

    elapsed = (time.time() - start) * 1000  # ms

    return topk_ids, distances[topk_ids], len(features), elapsed
```

**VÃ­ dá»¥:**

```python
query = features[0]  # T-shirt
distances = [0.000, 0.234, 0.156, ..., 0.892]  # 500 values

# Sort
sorted_ids = [0, 10, 25, 5, ...]  # Indices sorted by distance
topk_ids = sorted_ids[:10]  # Top-10
```

**Bottleneck:** Pháº£i tÃ­nh 500 distances â†’ cháº­m!

---

#### **3.3. LSH Search**

```python
def search_lsh(query_vec, features, index, metric, k):
    """
    LSH search - fast

    Steps:
    1. Query LSH index â†’ candidates (80 áº£nh)
    2. Compute distance chá»‰ vá»›i candidates
    3. Sort & return top-K

    Complexity: O(k) vá»›i kâ‰ˆ80 << 500
    """
    start = time.time()

    # Query LSH index
    candidates = index.query(query_vec)  # ~80 IDs

    if not candidates:
        # Fallback to linear
        return search_linear(query_vec, features, metric, k)

    # Get candidate features
    cand_ids = list(candidates)
    cand_feats = features[cand_ids]  # (80, 2560)

    # Compute distances chá»‰ vá»›i candidates
    distances = pairwise_distance(query_vec, cand_feats, metric)

    # Top-K trong candidates
    local_topk = topk_indices(distances, min(k, len(distances)))

    # Map back to global IDs
    topk_ids = np.array([cand_ids[i] for i in local_topk])
    topk_dists = distances[local_topk]

    elapsed = (time.time() - start) * 1000

    return topk_ids, topk_dists, len(candidates), elapsed
```

**VÃ­ dá»¥:**

```python
# Linear search
Compute 500 distances â†’ 30ms

# LSH search
Query index â†’ 0.5ms
Compute 80 distances â†’ 5ms
Total â†’ 5.5ms

Speedup = 30 / 5.5 â‰ˆ 5.5x
```

**Thá»±c táº¿ speedup cao hÆ¡n (19-22x) vÃ¬:**

- LSH query ráº¥t nhanh (hash lookup)
- Vectorized operations vá»›i Ã­t candidates

---

#### **3.4. Build Pipeline**

**File: `build.py`**

```python
def load_dataset(root_dir):
    """
    Scan dataset folder

    Structure:
    dataset/
      â”œâ”€â”€ T-shirt/0.jpg, 1.jpg, ...
      â”œâ”€â”€ Trouser/0.jpg, ...
      â””â”€â”€ ...

    Returns:
        paths: ["dataset/T-shirt/0.jpg", ...]
        labels: [0, 0, 0, ..., 1, 1, ...]
        class_names: ["T-shirt", "Trouser", ...]
    """
    root = Path(root_dir)
    if not root.exists():
        raise ValueError(f"Dataset not found: {root_dir}")

    class_dirs = sorted([d for d in root.iterdir() if d.is_dir()])
    class_names = [d.name for d in class_dirs]

    paths = []
    labels = []

    for label, class_dir in enumerate(class_dirs):
        imgs = sorted(class_dir.glob("*.jpg"))
        for img_path in imgs:
            paths.append(str(img_path))
            labels.append(label)

    return paths, labels, class_names
```

```python
def main():
    """Build features + LSH index"""

    # 1. Load dataset
    paths, labels, class_names = load_dataset(args.dataset)
    print(f"Found {len(paths)} images, {len(class_names)} classes")

    # 2. Extract features
    features = []
    for path in tqdm(paths, desc="Extracting"):
        feat = extract_feature(path, cfg)
        features.append(feat)

    features = np.array(features)  # (500, 2560)
    print(f"Shape: {features.shape}")

    # 3. Save features
    np.save(cfg.features_path, features)

    # 4. Save metadata
    meta_df = pd.DataFrame({
        'id': range(len(paths)),
        'path': paths,
        'label': labels,
        'class_name': [class_names[l] for l in labels]
    })
    meta_df.to_csv(cfg.meta_path, index=False)

    # 5. Build LSH index
    index = LSHIndex(cfg.num_tables, cfg.num_planes, features.shape[1], cfg.lsh_seed)
    index.fit(features)
    index.save(cfg.index_path)

    print("DONE!")
```

**Cháº¡y:**

```bash
python build.py --dataset dataset
```

**Output:**

```
Found 500 images, 10 classes
Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:02<00:00,  7.97it/s]
Shape: (500, 2560)
Saved: artifacts/features.npy, artifacts/meta.csv
Saved: artifacts/lsh_index.pkl
DONE!
```

---

### **Demo cho ThÃ nh viÃªn 3:**

**1. So sÃ¡nh 3 metrics:**

```python
query = features[0]
database = features[1:]

chi2_dists = chi2_distance(query, database)
l1_dists = l1_distance(query, database)
l2_dists = l2_distance(query, database)

# Top-10 cho má»—i metric
print("ChiÂ²:", topk_indices(chi2_dists, 10))
print("L1:  ", topk_indices(l1_dists, 10))
print("L2:  ", topk_indices(l2_dists, 10))

# CÃ³ thá»ƒ khÃ¡c nhau! â†’ ChiÂ² tá»‘t nháº¥t cho histogram
```

**2. Speedup chart:**

```python
import matplotlib.pyplot as plt

times_linear = []
times_lsh = []

for i in range(50):
    query = features[i]

    # Linear
    _, _, _, t_linear = search_linear(query, features, 'chi2', 10)
    times_linear.append(t_linear)

    # LSH
    _, _, _, t_lsh = search_lsh(query, features, index, 'chi2', 10)
    times_lsh.append(t_lsh)

plt.boxplot([times_linear, times_lsh], labels=['Linear', 'LSH'])
plt.ylabel('Time (ms)')
plt.title(f'Speedup: {np.mean(times_linear)/np.mean(times_lsh):.1f}x')
plt.show()
```

---

## ğŸ‘¤ THÃ€NH VIÃŠN 4: Evaluation + GUI

### **Nhiá»‡m vá»¥:**

- Implement Precision@K, Recall@K
- Compare Linear vs LSH
- Build GUI demo vá»›i Tkinter

### **File phá»¥ trÃ¡ch:** `eval.py`, `gui.py`

---

#### **4.1. Evaluation Metrics**

##### **Precision@K**

**Äá»‹nh nghÄ©a:**

```
Precision@K = (Sá»‘ áº£nh Ä‘Ãºng trong top-K) / K
```

**VÃ­ dá»¥:**

```python
Query: T-shirt (class 0)
Top-10 results:
#1: T-shirt âœ…
#2: Shirt   âŒ
#3: T-shirt âœ…
#4: Coat    âŒ
#5: T-shirt âœ…
#6: Shirt   âŒ
#7: T-shirt âœ…
#8: PulloverâŒ
#9: T-shirt âœ…
#10: T-shirtâœ…

ÄÃºng: 6/10
Precision@10 = 6/10 = 0.60 = 60%
```

**Code:**

```python
def compute_precision(query_label, result_labels, k):
    """
    Precision@K

    Args:
        query_label: 0 (T-shirt)
        result_labels: [0, 5, 0, 3, 0, ...] top-K labels
        k: 10

    Returns:
        precision: 0.0-1.0
    """
    # Count correct predictions
    correct = sum([1 for label in result_labels[:k] if label == query_label])

    return correct / k
```

---

##### **Recall@K**

**Äá»‹nh nghÄ©a:**

```
Recall@K = (Sá»‘ áº£nh Ä‘Ãºng trong top-K) / (Tá»•ng sá»‘ áº£nh Ä‘Ãºng trong database)
```

**VÃ­ dá»¥:**

```python
Dataset cÃ³ 50 T-shirts
Top-10 cÃ³ 6 T-shirts

Recall@10 = 6 / 50 = 0.12 = 12%
```

**Code:**

```python
def compute_recall(query_label, result_labels, k, total_relevant):
    """
    Recall@K

    Args:
        query_label: 0
        result_labels: [0, 5, 0, ...] top-K
        k: 10
        total_relevant: 50 (total T-shirts)

    Returns:
        recall: 0.0-1.0
    """
    correct = sum([1 for label in result_labels[:k] if label == query_label])

    return correct / total_relevant
```

---

##### **Trade-off: Precision vs Recall**

```
K cÃ ng lá»›n:
- Precision giáº£m (nhiá»u áº£nh sai hÆ¡n)
- Recall tÄƒng (cover nhiá»u áº£nh Ä‘Ãºng hÆ¡n)

VD vá»›i T-shirt:
K=5:   Precision=80%, Recall=8%
K=10:  Precision=60%, Recall=12%
K=20:  Precision=40%, Recall=16%
```

---

#### **4.2. Evaluation Pipeline**

```python
def evaluate(features, meta_df, index, mode='linear', k=10, num_queries=50, metric='chi2'):
    """
    Evaluate search performance

    Args:
        features: (500, 2560)
        meta_df: DataFrame vá»›i class_name
        index: LSHIndex hoáº·c None
        mode: 'linear', 'lsh', 'both'
        k: Top-K
        num_queries: Sá»‘ query test
        metric: 'chi2', 'l1', 'l2'

    Returns:
        results: dict vá»›i precision, recall, time
    """
    # Random sample queries
    np.random.seed(cfg.eval_seed)
    query_ids = np.random.choice(len(features), num_queries, replace=False)

    precisions = []
    recalls = []
    times = []

    for qid in tqdm(query_ids, desc=f"Evaluating {mode}"):
        query_vec = features[qid]
        query_label = meta_df.iloc[qid]['label']

        # Count total relevant
        total_relevant = (meta_df['label'] == query_label).sum()

        # Search
        if mode == 'linear':
            topk_ids, _, _, search_time = search_linear(query_vec, features, metric, k)
        else:  # lsh
            topk_ids, _, _, search_time = search_lsh(query_vec, features, index, metric, k)

        # Get labels
        result_labels = meta_df.iloc[topk_ids]['label'].values

        # Compute metrics
        prec = compute_precision(query_label, result_labels, k)
        rec = compute_recall(query_label, result_labels, k, total_relevant)

        precisions.append(prec)
        recalls.append(rec)
        times.append(search_time)

    return {
        'precision': np.mean(precisions),
        'recall': np.mean(recalls),
        'time': np.mean(times)
    }
```

**Cháº¡y evaluation:**

```bash
python eval.py --k 10 --num_queries 50 --mode both --metric chi2
```

**Output:**

```
LINEAR MODE
  Precision@10: 0.7200 (72%)
  Recall@10:    0.1440 (14.4%)
  Avg time:     28.5ms

LSH MODE
  Precision@10: 0.7200 (72%)  â† Same!
  Recall@10:    0.1440 (14.4%)
  Avg time:     1.5ms

SPEEDUP: 19.0x âš¡
```

**Observations:**

- Precision/Recall giá»‘ng nhau â†’ LSH khÃ´ng máº¥t accuracy
- Time giáº£m 19x â†’ LSH ráº¥t hiá»‡u quáº£!

---

#### **4.3. GUI Implementation**

**File: `gui.py`**

```python
class CBIRGUI:
    def __init__(self, root):
        """Initialize GUI"""
        self.root = root
        self.root.title("CBIR - Content-Based Image Retrieval")
        self.root.geometry("1200x700")

        # Load artifacts
        self.load_artifacts()

        # Create UI
        self.create_widgets()

        self.query_image_path = None
```

**Load artifacts:**

```python
def load_artifacts(self):
    """Load features, metadata, LSH index"""
    try:
        # Load features (500, 2560)
        self.features = np.load(cfg.features_path)

        # Load metadata (CSV manually vÃ¬ pandas cháº­m)
        self.meta = []
        with open(cfg.meta_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                self.meta.append(row)

        # Load LSH index
        self.lsh_index = LSHIndex.load(cfg.index_path)

        messagebox.showinfo("Success", f"Loaded {len(self.features)} images")
    except Exception as e:
        messagebox.showerror("Error", f"Cannot load artifacts:\n{e}")
        self.root.quit()
```

**UI Layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Image     [Chá»n áº¢nh Query]    ğŸ” Search     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚         â”‚     â—‹ LSH (Nhanh)     Top-K: [10]     â”‚
â”‚  â”‚ Preview â”‚     â—‹ Linear (ChÃ­nh xÃ¡c)              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚  âœ“ Found 10 results in 1.74ms (LSH mode)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Results:                                            â”‚
â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”                    â”‚
â”‚ â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚ â”‚ 4 â”‚ â”‚ 5 â”‚                    â”‚
â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜                    â”‚
â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”                    â”‚
â”‚ â”‚ 6 â”‚ â”‚ 7 â”‚ â”‚ 8 â”‚ â”‚ 9 â”‚ â”‚10 â”‚                    â”‚
â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Search function:**

```python
def search(self):
    """Perform search when button clicked"""
    if not self.query_image_path:
        messagebox.showwarning("Warning", "Please select query image!")
        return

    try:
        # Extract features
        query_feat = extract_feature(self.query_image_path, cfg)

        # Get settings
        mode = self.search_mode.get()  # 'lsh' or 'linear'
        topk = self.topk_var.get()     # 10

        # Search
        if mode == "lsh":
            indices, distances, num_cand, search_time = search_lsh(
                query_feat, self.features, self.lsh_index, 'chi2', topk
            )
        else:
            indices, distances, num_cand, search_time = search_linear(
                query_feat, self.features, 'chi2', topk
            )

        # Display results
        self.display_results(indices, distances, search_time, mode)

    except Exception as e:
        messagebox.showerror("Error", f"Search error:\n{e}")
```

**Display results:**

```python
def display_results(self, indices, distances, search_time, mode):
    """Show results in grid"""
    # Clear previous
    for widget in self.results_container.winfo_children():
        widget.destroy()

    # Update info
    self.info_label.config(
        text=f"âœ“ Found {len(indices)} results in {search_time:.2f}ms ({mode.upper()} mode)"
    )

    # Display in 5-column grid
    cols = 5
    for idx, (i, dist) in enumerate(zip(indices, distances)):
        row = idx // cols
        col = idx % cols

        # Frame for each result
        result_frame = tk.Frame(self.results_container, relief=tk.RAISED, bd=2)
        result_frame.grid(row=row, column=col, padx=5, pady=5)

        # Load & display image
        img_path = self.meta[i]['path']
        img = Image.open(img_path)
        img.thumbnail((150, 150))
        photo = ImageTk.PhotoImage(img)

        img_label = tk.Label(result_frame, image=photo)
        img_label.image = photo  # Keep reference!
        img_label.pack()

        # Info
        class_name = self.meta[i]['class_name']
        tk.Label(result_frame, text=f"#{idx+1}: {class_name}",
                 font=("Arial", 9, "bold")).pack()
        tk.Label(result_frame, text=f"Distance: {dist:.3f}",
                 font=("Arial", 8), fg="gray").pack()
```

**Cháº¡y GUI:**

```bash
python gui.py
```

---

### **Demo cho ThÃ nh viÃªn 4:**

**1. Precision/Recall curves:**

```python
import matplotlib.pyplot as plt

ks = [1, 3, 5, 10, 20, 30, 50]
precisions = []
recalls = []

for k in ks:
    results = evaluate(features, meta_df, index, 'lsh', k, 50, 'chi2')
    precisions.append(results['precision'])
    recalls.append(results['recall'])

plt.plot(recalls, precisions, 'o-')
plt.xlabel('Recall@K')
plt.ylabel('Precision@K')
plt.title('Precision-Recall Curve')
plt.grid()
plt.show()
```

**2. Compare metrics:**

```python
metrics = ['chi2', 'l1', 'l2']
results = {}

for metric in metrics:
    r = evaluate(features, meta_df, index, 'lsh', 10, 50, metric)
    results[metric] = r['precision']

plt.bar(metrics, [results[m] for m in metrics])
plt.ylabel('Precision@10')
plt.title('Metric Comparison')
plt.show()
```

---

## ğŸ“Š Tá»•ng káº¿t luá»“ng hoáº¡t Ä‘á»™ng

### **Phase 1: BUILD (Offline)**

```
1. Load Dataset (build.py)
   dataset/T-shirt/*.jpg â†’ paths, labels

2. Extract Features (ThÃ nh viÃªn 1 + 2)
   - HSV Color Histogram (ThÃ nh viÃªn 1): 2304-dim
   - LBP Texture (ThÃ nh viÃªn 2): 256-dim
   â†’ Combine = 2560-dim vector

3. Build LSH Index (ThÃ nh viÃªn 3)
   500 vectors â†’ 8 hash tables

4. Save Artifacts (ThÃ nh viÃªn 4)
   features.npy, meta.csv, lsh_index.pkl
```

### **Phase 2: SEARCH (Online)**

```
1. Query Image Input
   Chá»n query áº£nh tá»« dataset

2. Extract Features (ThÃ nh viÃªn 1 + 2)
   - HSV histogram (ThÃ nh viÃªn 1)
   - LBP histogram (ThÃ nh viÃªn 2)
   â†’ Query vector 2560-dim

3. LSH Query (ThÃ nh viÃªn 3)
   Hash query â†’ ~80 candidates

4. Compute Distance (ThÃ nh viÃªn 4)
   ChiÂ² distance vá»›i candidates

5. Sort & Return Top-K (ThÃ nh viÃªn 4)
   Top-10 smallest distances
```

### **Phase 3: EVALUATION (Offline)**

```
1. Random Sample Queries (ThÃ nh viÃªn 4)
   50 query images

2. Run Search (ThÃ nh viÃªn 4)
   - Linear search: 500 vectors
   - LSH search: ~80 candidates

3. Compute Metrics (ThÃ nh viÃªn 4)
   Precision@K, Recall@K, Time

4. Compare Performance (ThÃ nh viÃªn 4)
   Speedup: Linear vs LSH
```

---

## ğŸ¯ CÃ¢u há»i thÆ°á»ng gáº·p

### **Q1: Táº¡i sao dÃ¹ng Fashion-MNIST thay vÃ¬ CIFAR-10?**

A: Fashion-MNIST nháº¹ hÆ¡n (30MB vs 170MB), download nhanh, vÃ  váº«n Ä‘á»§ thÃº vá»‹ cho demo.

### **Q2: Táº¡i sao grid 3Ã—3 cho HSV?**

A: Trade-off giá»¯a spatial information vÃ  feature dimension:

- 1Ã—1 (global): Máº¥t thÃ´ng tin vá»‹ trÃ­
- 3Ã—3: Äá»§ capture "trÃªn/dÆ°á»›i", dimension khÃ´ng quÃ¡ lá»›n
- 5Ã—5: QuÃ¡ chi tiáº¿t, dimension explode

### **Q3: Táº¡i sao 8 tables, 12 planes?**

A: Tuning based on experiments:

- 4 tables: Miss nhiá»u candidates
- 8 tables: Balance tá»‘t
- 16 tables: Cháº­m hÆ¡n, khÃ´ng cáº£i thiá»‡n nhiá»u

### **Q4: LSH cÃ³ luÃ´n nhanh hÆ¡n Linear?**

A: KhÃ´ng! Náº¿u n nhá» (< 100 áº£nh), Linear cÃ³ thá»ƒ nhanh hÆ¡n vÃ¬ LSH cÃ³ overhead.

### **Q5: LÃ m sao improve accuracy?**

A:

1. ThÃªm features: SIFT, HOG, deep features (CNN)
2. TÄƒng grid: 3Ã—3 â†’ 4Ã—4
3. Feature fusion: Combine multiple features
4. Re-ranking: Spatial verification

---

## ğŸ“ Checklist trÆ°á»›c bÃ¡o cÃ¡o

### **Táº¥t cáº£ thÃ nh viÃªn:**

- [ ] Code Ä‘Ã£ push lÃªn GitHub
- [ ] Comment Ä‘áº§y Ä‘á»§ trong code
- [ ] Hiá»ƒu rÃµ code cá»§a mÃ¬nh (giáº£i thÃ­ch tá»«ng dÃ²ng)
- [ ] Test code: `python build.py`, `python search.py`, `python eval.py`

### **ThÃ nh viÃªn 1 - HSV Color (~60 lines):**

- [ ] Demo histogram visualization cho 3 classes
- [ ] So sÃ¡nh spatial vs global histogram
- [ ] Giáº£i thÃ­ch quantization: Táº¡i sao 16Ã—4Ã—4 bins?
- [ ] Tráº£ lá»i: HSV tá»‘t hÆ¡n RGB nhÆ° tháº¿ nÃ o?

### **ThÃ nh viÃªn 2 - LBP Texture (~40 lines):**

- [ ] Demo LBP codes visualization
- [ ] So sÃ¡nh texture: Coat (rough) vs Dress (smooth)
- [ ] Giáº£i thÃ­ch 8-neighbor encoding
- [ ] Tráº£ lá»i: LBP capture texture pattern ra sao?

### **ThÃ nh viÃªn 3 - LSH Indexing (83 lines):**

- [ ] Demo collision probability test
- [ ] Chart candidates reduction: 500 â†’ 80
- [ ] Giáº£i thÃ­ch random hyperplanes
- [ ] Tráº£ lá»i: Táº¡i sao 8 tables? Táº¡i sao 12 planes?

### **ThÃ nh viÃªn 4 - Search/Eval/Build (332 lines):**

- [ ] So sÃ¡nh 3 metrics: ChiÂ² vs L1 vs L2
- [ ] Speedup chart: Linear vs LSH (~19x)
- [ ] Precision-Recall curve cho K khÃ¡c nhau
- [ ] Tráº£ lá»i: Trade-off giá»¯a accuracy vÃ  speed?

**LÆ°u Ã½:** File `gui.py` (228 lines) lÃ  **optional bonus** cho demo trá»±c quan, **khÃ´ng báº¯t buá»™c** trong phÃ¢n cÃ´ng!

---

## ğŸš€ Má»Ÿ rá»™ng trong tÆ°Æ¡ng lai

1. **Deep Features:** DÃ¹ng CNN (ResNet, VGG) thay vÃ¬ HSV+LBP
2. **Re-ranking:** Spatial verification, query expansion
3. **Scalability:** Test vá»›i 1M áº£nh
4. **Web App:** Deploy lÃªn Flask/FastAPI
5. **Mobile App:** Android/iOS vá»›i TensorFlow Lite

---

**Good luck! ğŸ’ª**
